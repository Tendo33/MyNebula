# ==================== 应用配置 ====================
APP_NAME=mynebula
APP_VERSION=0.1.0
DEBUG=true


# ==================== 日志配置 ====================
LOG_LEVEL=INFO
LOG_FILE=logs/app.log

# ==================== 数据库配置 (PostgreSQL + pgvector) ====================
# Docker Compose 默认配置
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=mynebula
DATABASE_PASSWORD=mynebula_secret
DATABASE_NAME=mynebula

# 或使用完整 URL (优先级更高)
# DATABASE_URL=postgresql+asyncpg://mynebula:mynebula_secret@localhost:5432/mynebula

# ==================== GitHub 配置 ====================
# Personal Access Token (用于同步星标)
# 在 https://github.com/settings/tokens 创建
# 需要权限: public_repo, read:user, user:email
GITHUB_TOKEN=your_github_personal_access_token


# ==================== Embedding 配置 (OpenAI 兼容接口) ====================
EMBEDDING_API_KEY=your_embedding_api_key
EMBEDDING_BASE_URL=https://api.siliconflow.cn/v1
EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
EMBEDDING_DIMENSIONS=1024

# ==================== 预设提供商参考 ====================
#
# --- SiliconFlow (推荐国内用户，性价比高) ---
# EMBEDDING_PROVIDER=siliconflow
# EMBEDDING_BASE_URL=https://api.siliconflow.cn/v1
# EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
# EMBEDDING_DIMENSIONS=1024
#
# --- Jina AI ---
# EMBEDDING_PROVIDER=jina
# EMBEDDING_BASE_URL=https://api.jina.ai/v1
# EMBEDDING_MODEL=jina-embeddings-v3
# EMBEDDING_DIMENSIONS=1024
#
# --- OpenAI ---
# EMBEDDING_PROVIDER=openai
# EMBEDDING_BASE_URL=https://api.openai.com/v1
# EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_DIMENSIONS=1536
#
# --- 智谱 AI ---
# EMBEDDING_PROVIDER=zhipu
# EMBEDDING_BASE_URL=https://open.bigmodel.cn/api/paas/v4
# EMBEDDING_MODEL=embedding-3
# EMBEDDING_DIMENSIONS=2048
#
# --- Ollama (本地部署) ---
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_DIMENSIONS=768
# EMBEDDING_API_KEY=ollama  # Ollama 不需要真实 key，但字段不能为空

# ==================== LLM 配置 (OpenAI 兼容接口，用于生成摘要和聚类名称) ====================
LLM_API_KEY=your_llm_api_key
LLM_BASE_URL=https://api.siliconflow.cn/v1
LLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# ==================== 同步配置 ====================
# Star 同步的批次大小
SYNC_BATCH_SIZE=100
# README 获取的最大长度 (字符)
README_MAX_LENGTH=10000

# ==================== Docker Compose 配置 ====================
# 后端 API 端口映射
API_PORT=8000
# 前端端口映射
FRONTEND_PORT=3000
# 前端连接的 API 地址 (Docker Compose 部署时修改)
VITE_API_BASE_URL=http://localhost:8000
